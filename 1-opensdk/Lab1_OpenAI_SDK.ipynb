{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1db95a",
   "metadata": {},
   "source": [
    "# üß† Lab 1: Using the OpenAI SDK Directly (Without Frameworks)\n",
    "Welcome to your first **Agentic AI practical lab**!\n",
    "\n",
    "In this notebook, you'll learn how to interact directly with OpenAI's SDK using both:\n",
    "- ‚úÖ The new **Responses API**\n",
    "- ‚úÖ The classic **Chat Completions API**\n",
    "\n",
    "We'll explore parameters like `temperature`, `max_tokens`, and `top_p` to see how they affect outputs.\n",
    "\n",
    "> üí° Make sure your API key is set as an environment variable before starting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1faab9",
   "metadata": {},
   "source": [
    "### Step 1Ô∏è‚É£ ‚Äì Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1491c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file if available\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309e4074",
   "metadata": {},
   "source": [
    "### Step 2Ô∏è‚É£ ‚Äì Check the OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94280666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f'‚úÖ OpenAI API Key exists and begins {openai_api_key[:8]}')\n",
    "else:\n",
    "    print('‚ùå API Key not set ‚Äì please check your .env or environment variables.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e817d60",
   "metadata": {},
   "source": [
    "### Step 3Ô∏è‚É£ ‚Äì Initialize the OpenAI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f624d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "print('Client initialized successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a922886",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Part A: Using the **New Responses API**\n",
    "\n",
    "This is the modern interface for OpenAI models. It uses `instructions` + `input` instead of chat-style messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604c0a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Explain what list comprehensions are in Python with an example.'\n",
    "\n",
    "response = client.responses.create(\n",
    "    model='gpt-4o',\n",
    "    instructions='You are a helpful Python instructor.',\n",
    "    input=prompt\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ad0a58",
   "metadata": {},
   "source": [
    "### üî¨ Experiment with Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee1cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Write a haiku about Python programming.'\n",
    "for temp in [0.2, 0.8]:\n",
    "    res = client.responses.create(\n",
    "        model='gpt-4o-mini',\n",
    "        instructions='You are a poetic assistant.',\n",
    "        input=prompt,\n",
    "        temperature=temp,\n",
    "        max_output_tokens=50\n",
    "    )\n",
    "    print(f'\\n--- Temperature {temp} ---\\n{res.output_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4a3b68",
   "metadata": {},
   "source": [
    "### üìä Inspect the Response Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b1bc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model:', response.model)\n",
    "print('Usage:', response.usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fdb0c4",
   "metadata": {},
   "source": [
    "### üß© Create a Reusable Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e809ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_responses(prompt, temperature=0.7, max_tokens=150):\n",
    "    '''Send a query using the new Responses API.'''\n",
    "    resp = client.responses.create(\n",
    "        model='gpt-4o-mini',\n",
    "        instructions='You are a concise coding tutor.',\n",
    "        input=prompt,\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_tokens\n",
    "    )\n",
    "    return resp.output_text\n",
    "\n",
    "print(ask_responses('Summarize what the with statement does in Python.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504a65f1",
   "metadata": {},
   "source": [
    "### üí° Structured Output (JSON Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189c1ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_json = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input='List three Python data types and their use cases in JSON format.',\n",
    "    temperature=0.5,\n",
    "    response_format={'type': 'json_object'}\n",
    ")\n",
    "print(response_json.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cffa1b2",
   "metadata": {},
   "source": [
    "## üí¨ Part B: Using the **Classic Chat Completions API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047002bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "messages = [\n",
    "    {'role': 'system', 'content': 'You are a friendly Python assistant.'},\n",
    "    {'role': 'user', 'content': 'Explain what decorators are in Python with an example.'}\n",
    "]\n",
    "\n",
    "chat_response = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=messages\n",
    ")\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4ef1bc",
   "metadata": {},
   "source": [
    "### üéõÔ∏è Chat API Parameters Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2727f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in [0.2, 0.9]:\n",
    "    r = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{'role': 'user', 'content': 'Write a short poem about recursion.'}],\n",
    "        temperature=temp,\n",
    "        max_tokens=60\n",
    "    )\n",
    "    print(f'\\n--- Temperature {temp} ---\\n{r.choices[0].message.content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc5ab4f",
   "metadata": {},
   "source": [
    "### üß© Reusable Function for Chat API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874392a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_chat(prompt, temperature=0.7, max_tokens=150):\n",
    "    '''Send a prompt using the Chat Completions API.'''\n",
    "    r = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return r.choices[0].message.content\n",
    "\n",
    "print(ask_chat('Summarize what a Python lambda function is used for.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b59f60",
   "metadata": {},
   "source": [
    "## üß† Key Takeaways\n",
    "- **Responses API** ‚Üí Modern and simpler (`instructions` + `input`)\n",
    "- **Chat Completions API** ‚Üí Older but still widely supported (`messages`)\n",
    "- Parameters behave identically in both.\n",
    "- Use `Responses API` for new projects.\n",
    "\n",
    "Next Lab ‚Üí *Building simple memory and chaining LLM calls.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
