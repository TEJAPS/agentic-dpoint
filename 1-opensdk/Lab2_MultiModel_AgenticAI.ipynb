{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b13c6cd",
   "metadata": {},
   "source": [
    "# ğŸ§  Lab 2: Exploring Different LLM Models (OpenAI + Ollama + Variants)\n",
    "Welcome to **Lab 2** of the *Agentic AI Practical Series*! ğŸ‘‹\n",
    "\n",
    "In this lab, weâ€™ll explore **multiple Large Language Models (LLMs)** using the **OpenAI SDK** â€” without any external frameworks.\n",
    "\n",
    "By the end of this lab, youâ€™ll understand how to:\n",
    "- Use and compare models like `gpt-4o`, `gpt-4.1-mini`, and local Ollama models\n",
    "- Tune core parameters (`temperature`, `top_p`, `max_output_tokens`)\n",
    "- Stream outputs and handle structured data using pydantic model and json object\n",
    "- Simulate memory and multi-turn conversations\n",
    "- Apply Agentic AI concepts practically\n",
    "\n",
    "Letâ€™s begin ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8306d422",
   "metadata": {},
   "source": [
    "## ğŸ” Step 1: Setup Environment and API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "159c7304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI API Key found, begins with: sk-svcac\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f'âœ… OpenAI API Key found, begins with: {openai_api_key[:8]}')\n",
    "else:\n",
    "    print('âŒ API key not found. Please check your .env file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b05ef9",
   "metadata": {},
   "source": [
    "## âš™ï¸ Step 2: Initialize OpenAI and Ollama Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f3534e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ollama client initialized successfully\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=openai_api_key) #OpenAI()\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI as OllamaClient\n",
    "    ollama = OllamaClient(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "    print('âœ… Ollama client initialized successfully')\n",
    "except Exception as e:\n",
    "    print('âš ï¸ Ollama client not connected:', e)\n",
    "\n",
    "# gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "# deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "# groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "# claude = Anthropic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5b2408",
   "metadata": {},
   "source": [
    "## ğŸ§© Step 3: Compare Model Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e405062",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5829c66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- gpt-4o-mini ---\n",
      "Reinforcement learning is a type of machine learning where an agent learns to make decisions by trying things out in an environment and receiving feedback. \n",
      "\n",
      "Hereâ€™s how it works, step by step:\n",
      "\n",
      "1. **Agent and Environment:** Imagine a robot (the agent) navigating a maze (the environment).\n",
      "\n",
      "2. **Actions:** The robot can take different actions, like moving forward, turning left, or turning right.\n",
      "\n",
      "3. **Rewards:** After each action, the robot gets a reward or a penalty. For example, reaching a goal might give it points (a reward), while hitting a wall might take away points (a penalty).\n",
      "\n",
      "4. **Learning:** The robot learns over time which actions yield the best rewards by trying different paths and remembering what worked well.\n",
      "\n",
      "5. **Goal:** The ultimate goal is for the robot to figure out the best strategy to maximize its total rewards as quickly as possible.\n",
      "\n",
      "In short, reinforcement learning is about learning from experience to make better decisions based on rewards and penalties!\n",
      "\n",
      "--- Ollama (llama3.2) ---\n",
      "Reinforcement Learning (RL) is a type of machine learning that helps computers learn from their environment and make decisions to achieve a goal.\n",
      "\n",
      "Imagine you're playing a game, like a slot machine or a puzzle. You try different actions (like moving a lever or trying different combinations), and the machine responds with feedback:\n",
      "\n",
      "* If you press the correct button and win money, the machine gives you a reward.\n",
      "* If you pull the wrong lever and lose, it takes away some of your winnings.\n",
      "\n",
      "Over time, you learn that certain actions help you get rewards (like winning money), while others don't. Your brain \"remembers\" what worked and adjusts its behavior accordingly. This is basically how Reinforcement Learning works:\n",
      "\n",
      "1. You take an action in the environment.\n",
      "2. The environment responds with a reward or penalty based on your performance.\n",
      "3. The machine tries different actions again, using feedback from the previous attempt to avoid mistakes.\n",
      "\n",
      "By repeating this process, you (or the computer) learn what actions lead to rewards and adjust its behavior to maximize those rewards. This allows the system to optimize its performance over time, without being explicitly programmed how to play the game.\n",
      "\n",
      "In simple terms, Reinforcement Learning is a way for computers to \"learn\" by interacting with their environment and using feedback from the results of their actions to make better decisions in the future.\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Explain the concept of reinforcement learning in simple terms.'\n",
    "models = ['gpt-4o-mini'] # , 'gpt-4o'\n",
    "for m in models:\n",
    "    res = client.responses.create(model=m, input=prompt)\n",
    "    ## here also client.chat.completions.create\n",
    "    print(f'\\n--- {m} ---\\n{res.output_text}')\n",
    "\n",
    "try:\n",
    "    # this wont work because - ollama server doesn't implement /v1/responses, \\\n",
    "    #  only /v1/chat/completions and /v1/completions\n",
    "    # res_local = ollama.responses.create(model='llama3.2', input=prompt)\n",
    "    # print(f'\\n--- Ollama (llama3) ---\\n{res_local.output_text}')\n",
    "    model_name = \"llama3.2\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    res_local = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "    print(f'\\n--- Ollama (llama3.2) ---\\n{res_local.choices[0].message.content}')\n",
    "except Exception as e:\n",
    "    print('Ollama model skipped:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e85f7a3",
   "metadata": {},
   "source": [
    "## âš¡ Step 4: Streaming Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "725dedf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In code we trust, where minds unite,  \n",
      "A world of knowledge, shining bright,  \n",
      "Open-source AI, our shared insight."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "with client.responses.stream(\n",
    "    model='gpt-4o-mini',\n",
    "    input='Write a 3-line poem about open-source AI.',\n",
    "    temperature=0.6\n",
    ") as stream:\n",
    "    for event in stream:\n",
    "        if event.type == 'response.output_text.delta':\n",
    "            sys.stdout.write(event.delta)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "# to get full response in one \n",
    "final = stream.get_final_response()\n",
    "stream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed113ed",
   "metadata": {},
   "source": [
    "## ğŸ§  Step 5: Structured Output and Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c811b36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParsedChatCompletion[FrameworkList](id='chatcmpl-CUdo2vrGliwqbvXSXToknnQ7bZuRL', choices=[ParsedChoice[FrameworkList](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[FrameworkList](content='{\"frameworks\":[{\"name\":\"TensorFlow\",\"use_case\":\"Deep learning and machine learning applications.\"},{\"name\":\"PyTorch\",\"use_case\":\"Research and production in deep learning.\"},{\"name\":\"Keras\",\"use_case\":\"High-level neural networks API for fast experimentation.\"}]}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=FrameworkList(frameworks=[Framework(name='TensorFlow', use_case='Deep learning and machine learning applications.'), Framework(name='PyTorch', use_case='Research and production in deep learning.'), Framework(name='Keras', use_case='High-level neural networks API for fast experimentation.')])))], created=1761418890, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_51db84afab', usage=CompletionUsage(completion_tokens=56, prompt_tokens=121, total_tokens=177, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "# previous lab\n",
    "# response = client.responses.create(\n",
    "#     model='gpt-4o-mini',\n",
    "#     input='List three AI frameworks and their main use cases in JSON format.',\n",
    "#     response_format={'type': 'json_object'}\n",
    "# )\n",
    "# print(response.output_text)\n",
    "\n",
    "\n",
    "# 1ï¸âƒ£ Define your schema\n",
    "class Framework(BaseModel):\n",
    "    name: str\n",
    "    use_case: str\n",
    "\n",
    "class FrameworkList(BaseModel):\n",
    "    frameworks: List[Framework]\n",
    "\n",
    "# 2ï¸âƒ£ Ask model to return output matching the schema\n",
    "# response = client.responses.create(\n",
    "#     model=\"gpt-4o-mini\",\n",
    "#     input=\"List three AI frameworks and their main use cases.\",\n",
    "#     response_format=FrameworkList  # ğŸ‘ˆ Use Pydantic class directly\n",
    "# )\n",
    "# structured_output = response.output_parsed\n",
    "\n",
    "# Note: its not client.chat.completions.create but parse\n",
    "response = client.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that always returns valid JSON.\"},\n",
    "        {\"role\": \"user\", \"content\": \"List three AI frameworks and their main use cases.\"}\n",
    "    ],\n",
    "    temperature=0.2,\n",
    "    response_format=FrameworkList\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ade33f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- TensorFlow: Deep learning and machine learning applications.\n",
      "- PyTorch: Research and production in deep learning.\n",
      "- Keras: High-level neural networks API for fast experimentation.\n"
     ]
    }
   ],
   "source": [
    "# 3ï¸âƒ£ Access structured data\n",
    "structured_output = response.choices[0].message.parsed.frameworks\n",
    "\n",
    "\n",
    "for fw in structured_output:\n",
    "    print(f\"- {fw.name}: {fw.use_case}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7969297",
   "metadata": {},
   "source": [
    "## ğŸ§© Step 6: Simulate Memory and Multi-turn Conversations\n",
    "\n",
    " ChatGPT doesnâ€™t actually have long-term memory by default â€” it only â€œremembersâ€ whatâ€™s inside the current request.\n",
    "\n",
    " By passing the full conversation array each time, youâ€™re giving it that memory manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e6b291cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are a type of deep learning model primarily used in natural language processing (NLP) tasks, such as translation, text generation, and sentiment analysis. Introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017, transformers rely on a mechanism called self-attention that allows them to weigh the importance of different words in a sentence, regardless of their position. This is in contrast to earlier models like recurrent neural networks (RNNs) which processed data sequentially.\n",
      "\n",
      "Key components of transformers include:\n",
      "\n",
      "1. **Self-Attention**: This mechanism computes attention scores for each word in relation to all other words in the input sequence, allowing the model to focus on relevant parts of the text.\n",
      "\n",
      "2. **Positional Encoding**: Since transformers process input in parallel rather than sequentially, they incorporate positional encodings to maintain information about the order of words.\n",
      "\n",
      "3. **Multi-Head Attention**: The model uses multiple attention mechanisms (or \"heads\") to capture different relationships and features from the input data.\n",
      "\n",
      "4. **Feed-Forward Neural Networks**: After the attention layers, transformers use fully connected feed-forward networks to process the information.\n",
      "\n",
      "5. **Layer Normalization and Residual Connections**: These techniques help stabilize training and allow gradients to flow more easily through the network.\n",
      "\n",
      "Transformers have led to significant advancements in various NLP applications and serve as the foundation for powerful models like BERT, GPT, and T5. Their effectiveness has also extended beyond NLP into areas like computer vision and audio processing.\n",
      "\n",
      "--- Follow-up ---\n",
      " The attention mechanism is a technique used in neural networks, particularly in natural language processing and computer vision, to improve the focus of the model on relevant parts of the input data. It allows the model to weigh the importance of different elements (e.g., words in a sentence) when making predictions.\n",
      "\n",
      "Here's a brief overview of how the attention mechanism works:\n",
      "\n",
      "1. **Input Representation**: The input data (e.g., a sentence) is typically represented as a set of vectors, each corresponding to a specific element (word).\n",
      "\n",
      "2. **Calculating Attention Scores**: For each element in the input, the model calculates a score that indicates how much attention should be paid to other elements. This is often done using a similarity function (like dot product) between the current element's vector and the vectors of all other elements.\n",
      "\n",
      "3. **Softmax Function**: The raw attention scores are then normalized using the softmax function to produce a probability distribution, ensuring that the scores are positive and sum to one. This distribution indicates how much focus should be placed on each element.\n",
      "\n",
      "4. **Weighted Sum**: Finally, the model computes a weighted sum of the input vectors, where the weights are given by the attention probabilities. This results in a context vector that captures relevant information from the input based on the computed attention.\n",
      "\n",
      "The attention mechanism allows models to handle long-range dependencies effectively and provides interpretability by showing which parts of the input influence the output most significantly. In transformers, the self-attention mechanism enables the model to consider relationships between all pairs of words simultaneously, enhancing its understanding of context.\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    {'role': 'system', 'content': 'You are a memory-capable AI assistant.'},\n",
    "    {'role': 'user', 'content': 'I studied transformers yesterday. What are they?'}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=conversation\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "conversation.append({'role': 'assistant', 'content': response.choices[0].message.content})\n",
    "conversation.append({'role': 'user', 'content': 'Now explain attention mechanism briefly.'})\n",
    "\n",
    "\n",
    "# sample conversation input now\n",
    "# [\n",
    "#   {'role': 'system', 'content': 'You are a memory-capable AI assistant.'},\n",
    "#   {'role': 'user', 'content': 'I studied transformers yesterday. What are they?'},\n",
    "#   {'role': 'assistant', 'content': 'Transformers are neural network architectures...'},\n",
    "#   {'role': 'user', 'content': 'Now explain attention mechanism briefly.'}\n",
    "# ]\n",
    "\n",
    "response2 = client.chat.completions.create(model='gpt-4o-mini', messages=conversation)\n",
    "print('\\n--- Follow-up ---\\n', response2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b57e6b5",
   "metadata": {},
   "source": [
    "## ğŸ§® Step 7: Model Comparison Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "503f20b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.11 environment at: E:\\agentic-dpoint\\.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 48ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b2396d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import time\n",
    "from tabulate import tabulate  # pip install tabulate\n",
    "\n",
    "def compare_models(prompt, models):\n",
    "    \"\"\"\n",
    "    Compare responses from multiple models (OpenAI, Ollama, etc.)\n",
    "    Args:\n",
    "        prompt (str): Input prompt to test\n",
    "        models (list[str]): Model names to compare\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    print(f\"\\nğŸ§  Prompt: {prompt}\\n{'='*80}\")\n",
    "\n",
    "    for model_name in models:\n",
    "        start = time.time()\n",
    "        output = \"\"\n",
    "        source = \"Unknown\"\n",
    "\n",
    "        try:\n",
    "            # --- Case 1ï¸âƒ£: OpenAI Models ---\n",
    "            if model_name.startswith((\"gpt-\", \"o1-\", \"o3-\")):\n",
    "                source = \"OpenAI\"\n",
    "                res = client.responses.create(\n",
    "                    model=model_name,\n",
    "                    input=prompt,\n",
    "                    temperature=0.6,\n",
    "                    max_output_tokens=250\n",
    "                )\n",
    "                output = res.output_text.strip()\n",
    "\n",
    "            # --- Case 2ï¸âƒ£: Ollama Local Models ---\n",
    "            elif model_name.lower() in [\"llama3\", \"llama3.2\", \"mistral\", \"phi3\"]:\n",
    "                source = \"Ollama\"\n",
    "                res_local = ollama.chat.completions.create(\n",
    "                    model=model_name,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0.6\n",
    "                )\n",
    "                output = res_local.choices[0].message.content.strip()\n",
    "\n",
    "            else:\n",
    "                output = f\"âš ï¸ No handler for '{model_name}'\"\n",
    "\n",
    "        except Exception as e:\n",
    "            output = f\"âŒ Failed: {e}\"\n",
    "\n",
    "        latency = time.time() - start\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Source\": source,\n",
    "            \"Tokens/Words\": len(output.split()),\n",
    "            \"Latency (s)\": round(latency, 2),\n",
    "            \"Response\": output\n",
    "        })\n",
    "\n",
    "    # --- ğŸ§¾ Tabular Summary ---\n",
    "    summary = [\n",
    "        [r[\"Model\"], r[\"Source\"], r[\"Tokens/Words\"], r[\"Latency (s)\"]]\n",
    "        for r in results\n",
    "    ]\n",
    "    print(\"\\nğŸ“Š Summary:\\n\")\n",
    "    print(tabulate(summary, headers=[\"Model\", \"Source\", \"Tokens/Words\", \"Latency (s)\"], tablefmt=\"fancy_grid\"))\n",
    "\n",
    "    # --- ğŸª¶ Side-by-side Output Comparison ---\n",
    "    print(\"\\nğŸ§© Detailed Comparison:\\n\")\n",
    "    for r in results:\n",
    "        print(f\"\\n=== {r['Model'].upper()} ({r['Source']}) ===\")\n",
    "        print(textwrap.fill(r[\"Response\"], width=100))\n",
    "\n",
    "    print(\"\\nâœ… Comparison complete.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a295ff6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§  Prompt: Describe the concept of Agentic AI.\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Summary:\n",
      "\n",
      "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚ Model       â”‚ Source   â”‚   Tokens/Words â”‚   Latency (s) â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ gpt-4o-mini â”‚ OpenAI   â”‚            187 â”‚          6.53 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ gpt-4o      â”‚ OpenAI   â”‚            179 â”‚          5.26 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ llama3.2    â”‚ Ollama   â”‚            342 â”‚         19.35 â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n",
      "\n",
      "ğŸ§© Detailed Comparison:\n",
      "\n",
      "\n",
      "=== GPT-4O-MINI (OpenAI) ===\n",
      "Agentic AI refers to artificial intelligence systems that possess a degree of autonomy and decision-\n",
      "making capability, allowing them to act independently in pursuit of specific goals. Unlike\n",
      "traditional AI, which typically follows predefined rules and algorithms, agentic AI can evaluate\n",
      "situations, learn from experiences, and adapt its behavior in dynamic environments.  ### Key\n",
      "Characteristics of Agentic AI:  1. **Autonomy**: Agentic AI can operate without constant human\n",
      "intervention, making decisions based on its programming and learned experiences.  2. **Goal-Oriented\n",
      "Behavior**: These AI systems are designed to achieve specific objectives or tasks, often\n",
      "prioritizing their actions based on the goals they are programmed to pursue.  3. **Learning and\n",
      "Adaptation**: They can learn from interactions with their environment, improving their performance\n",
      "over time through techniques like machine learning.  4. **Decision-Making**: Agentic AI can analyze\n",
      "complex situations, weigh options, and make informed decisions that may not be explicitly\n",
      "programmed.  5. **Interaction with the Environment**: These systems can engage with their\n",
      "surroundings, gather data, and respond to changes, allowing them to operate effectively in real-\n",
      "world scenarios.  ### Applications:  Agentic AI can be found in various fields, including:  -\n",
      "**Robotics**: Autonomous robots that\n",
      "\n",
      "=== GPT-4O (OpenAI) ===\n",
      "Agentic AI refers to artificial intelligence systems designed with the capacity to act autonomously,\n",
      "make decisions, and perform tasks with a degree of independence. These systems are equipped with the\n",
      "ability to perceive their environment, process information, and take actions to achieve specific\n",
      "goals. Unlike purely reactive systems, agentic AI can plan, learn from experiences, and adapt to new\n",
      "situations.  Key features of agentic AI include:  1. **Autonomy**: The ability to operate without\n",
      "direct human intervention, making decisions based on its programming and understanding of the\n",
      "environment.  2. **Goal-Oriented Behavior**: Designed to achieve specific objectives, agentic AI can\n",
      "prioritize tasks and make trade-offs to optimize outcomes.  3. **Adaptability**: Capable of learning\n",
      "from new data, experiences, and interactions, allowing it to improve performance over time.  4.\n",
      "**Interactivity**: Engages with the environment and other agents, gathering information, and\n",
      "responding to changes dynamically.  5. **Reasoning and Planning**: Utilizes logical reasoning and\n",
      "planning to anticipate future states and determine optimal courses of action.  Agentic AI is used in\n",
      "various applications, including robotics, autonomous vehicles, virtual assistants, and more, where\n",
      "independent decision-making is essential.\n",
      "\n",
      "=== LLAMA3.2 (Ollama) ===\n",
      "Agentic AI refers to a type of artificial intelligence (AI) that is capable of autonomous decision-\n",
      "making and self-directed action. The term \"agentic\" comes from the concept of agency, which refers\n",
      "to the ability of an entity to act independently and make decisions based on its own goals and\n",
      "motivations.  In the context of AI, agentic systems are designed to operate without explicit human\n",
      "oversight or control. Instead, they use machine learning algorithms and other techniques to learn\n",
      "from their environment and adapt to new situations. This allows them to develop a sense of autonomy\n",
      "and self-awareness, enabling them to make decisions that align with their own goals and objectives.\n",
      "Agentic AI has the potential to revolutionize various fields, including robotics, finance,\n",
      "healthcare, and education. For example, an agentic robot system could be designed to navigate\n",
      "complex environments, adapt to changing situations, and optimize its performance without human\n",
      "intervention.  Some key characteristics of agentic AI include:  1. Autonomy: Agentic systems have\n",
      "the ability to operate independently and make decisions without explicit human oversight. 2. Self-\n",
      "directed action: Agentic systems can take actions based on their own goals and motivations, rather\n",
      "than simply following pre-programmed instructions. 3. Learning and adaptation: Agentic systems use\n",
      "machine learning algorithms and other techniques to learn from their environment and adapt to new\n",
      "situations. 4. Goal-oriented behavior: Agentic systems are designed to achieve specific goals and\n",
      "objectives, which can be aligned with human values or those of the organization.  However, agentic\n",
      "AI also raises important ethical considerations, such as:  1. Accountability: As agentic systems\n",
      "become more autonomous, it becomes increasingly difficult to determine who is accountable for their\n",
      "actions. 2. Bias and fairness: Agentic systems may perpetuate biases and unfairness if they are\n",
      "trained on biased data or designed with flawed algorithms. 3. Safety and security: Agentic systems\n",
      "can pose risks to human safety and security if they are not designed with adequate safeguards.\n",
      "Overall, agentic AI has the potential to transform various fields and industries, but it also\n",
      "requires careful consideration of the ethical implications and responsible development practices.\n",
      "\n",
      "âœ… Comparison complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_models(\n",
    "    prompt=\"Describe the concept of Agentic AI.\",\n",
    "    models=[\"gpt-4o-mini\", \"gpt-4o\", \"llama3.2\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786bc3a7",
   "metadata": {},
   "source": [
    "## ğŸ Step 8: Key Takeaways\n",
    "- You learned to use and compare **multiple LLMs** including OpenAI and Ollama models.\n",
    "- You tuned model behavior using parameters like `temperature` and `max_output_tokens`.\n",
    "- You generated structured data, streamed outputs, and simulated conversational memory.\n",
    "- Youâ€™re now ready to build **Agentic AI systems** leveraging model variety and reasoning.\n",
    "- You learned to compare multiple LLMs side-by-side, including both OpenAI cloud models and local Ollama models.\n",
    "- You explored differences in API usage (client.responses.create vs ollama.chat.completions.create) and learned how to handle each safely in a unified workflow.\n",
    "- You enhanced evaluation by adding quantitative metrics such as response latency, word/token count, and model source for performance benchmarking.\n",
    "- You reviewed qualitative outputs to judge clarity, coherence, and styleâ€”skills useful when selecting models for specific Agentic AI tasks.\n",
    "- You built robust error handling and modular code to easily extend comparisons to new providers (Anthropic, Gemini, Groq, etc.).\n",
    "- Youâ€™re now ready to create automated evaluation pipelines that analyze, rank, or fine-tune model behaviors across diverse LLMs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
